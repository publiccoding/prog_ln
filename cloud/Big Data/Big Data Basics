Hadoop ecosystem 

Hive - SQL DB ( data analytics )
pig - scripting language ( data analytics )
scoope - export/import framework 
spark - inmemory computing framework database 
flume - data injection tool from web locks into hadoop ( data can be processed and )

Hadoop Installation:

1.hadoop.apache.org     
2.download the tar file and extract it cd hadoop-2.7.2 
    a. core-site.xml 
    <configuration>
    <property>
    <name> fs.default.name</name>
    <value>hdfs://localhost:9000</value>
    </property>
    </configuration>
    
    b.hdfs-site.xml 
    <configuration>
    <property>
    <name>dfs.replication</name>
    <value>1</value>
    </property>
    <property>
    <name>dfs.permission</name>
    <value>false</value>
    </property>
    <property>
    <name>dfs.namenode.name.dir</name>
    <value>/home/edureka/hadoop-2.7.2/hadoop2_data/hdfs/namenode</value>
    </property>
    <property>
    <name>dfs.datanode.data.dir</name>
    <value>/home/edureka/hadoop-2.7.2/hadoop2_data/hdfs/datanode</value>
    </property>
    </configuration>
    
    c.yarn-site.xml 
    <configuration>
    <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
    </property>
    <property>
    <name>yarn.nodemanager.auxservices.mapreduce_shuffle.class</name>
    <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    
    </configuration>
    

3.Copy mapred-site.xml.template to mapred-site.xml

    a. vi mapred-site.xml
    <configuration>
    <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
    </property>
    </configuration>
    b.vi hadoop-env.sh 
    export JAVA_HOME=$PATH of java home 
    c. vi .bashrc 
    export HADOOP_HOME=/home/edureka/hadoop-2.7.2
    export HADOOP_CONF=/home/edureka/hadoop-2.7.2/etc/hadoop
    export HADOOP_MAPRED_HOME=/home/edureka/hadoop-2.7.2
    export HADOOP_COMMON_HOME=/home/edureka/hadoop-2.7.2
    export HADOOP_HDFS_HOME=/home/edureka/hadoop-2.7.2
    export YARN_HOME=/home/edureka/hadoop-2.7.2
    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
    export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"

    export JAVA_HOME=/usr/lib/jvm/jdk1.7.0_67
    export PATH=$PATH:/usr/lib/jvm/jdk1.7.0_67/bin 

    export PATH=$PATH:/home/edureka/hadoop-2.7.2/bin
    export HADOOP_PID_DIR=/home/edureka/hadoop-2.7.2/hadoop2_data/hdfs/pid 
    d. source .bashrc

    e. Format name node 
    cd hadoop-2.7.2 
    /bin/hadoop namenode -format 
    cd hadoop-2.7.2 
    cd sbin
    ./start-dfs.sh < enter password > 
    ./start-yarn.sh < enter password> 
    ./mr-jobhistory-daemon.sh start historyserver 
    sudo jps   #50070 port 

hadoop files:

hadoop fs -ls /olympics
hadoop dfs -put <dir_name> <dir_name>
pig 
> olympic = load '/olympic/input/olympix_data.csv' using PigStorage('\t');
> dump olympic; 
> country_final = foreach olympic generate $2 as Country, $9 as total_medals;
> dump country_final;
> grouped = group Country_final by Country;
> final_result = foreach grouped generate group, COUNT(country_final.total_medals) as f_count;
> dump final_result
> sort = order final_result by f_count desc;
> final_count = limit sort 10 
> store final_count into '/olympic/output

particular sports 
olympic = load '/olympic/input/olympix_data.csv' using PigStorage('\t');
> dump olympic;
> country_final = foreach olympic generate $2 as Country, $5 as sports, $9 as total_medals;
> atheletes_filter = FILTER country_final by sport='Swimming'
> final_group = GROUP atheletes_filter by Country;
> dump final_group
> final_count = foreach final_group generate group, COUNT(atheletes_filter) as f_count;
> sort = order final_count by f_count desc ; 
> hadoop fs -cat /olympic/output/usercasefirst/*0


use Case1: 

> hadoop jar review.jar /project/<filename.xml> /project/out 
> a = load '/project/out/' using PigStorage(',') as (category:chararray, hash:chararray,url:chararray,postive:int,negative:int,totalreview:int);
> url_cat = foreach a generate hash, TRIM(category)
> store url_cat into '/project/url_cat/';
> store url_review_distict into '/project/url_review_distict/' ; 
 
Connect with Hive: 

hive 
> drop table url_cat ; 
> create external table URL_CAT( hash string, category string) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
> LOCATION '/project/url_cat/';
> LOAD DATA INPATH '/project/url_cat/part-m-00000' into table URL_CAT;
> select * from url_cat;
> drop table url_review_distict;































